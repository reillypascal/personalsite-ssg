---
title: AI-Critical Writing and Reporting
description: Reading list of sources critical of AI or reporting on harm caused by its use
fedi_url:
  - https://hachyderm.io/@reillypascal/114597601909960276
  - https://bsky.app/profile/reillypascal.bsky.social/post/3lqflxhpllc2j
date: 2025-06-12T18:47:00-0400
octothorpes:
  - ai
tags:
  - wiki
  - ai
  - politics
  - reading
---

This page is a collection of reporting, research, and essays critical of large language models (LLMs) and other “generative AI.” Articles are organized based on topic. It is an ever-growing list.

## AI as a Labor Issue

- Aaron Benanav, “[Is the AI Bubble About to Burst?](https://www.versobooks.com/blogs/news/is-the-ai-bubble-about-to-burst)”

> The real threat posed by generative AI is not that it will eliminate work on a mass scale, rendering human labour obsolete. It is that, left unchecked, it will continue to transform work in ways that deepen precarity, intensify surveillance, and widen existing inequalities. Technological change is not an external force to which societies must simply adapt; it is a socially and politically mediated process. Legal frameworks, collective bargaining, public investment, and democratic regulation all play decisive roles in shaping how technologies are developed and deployed, and to what ends.

- Noam Scheiber, “[At Amazon, Some Coders Say Their Jobs Have Begun to Resemble Warehouse Work](https://www.nytimes.com/2025/05/25/business/amazon-ai-coders.html)” ([archived link](https://archive.is/g127S))

> Three Amazon engineers said that managers had increasingly pushed them to use A.I. in their work over the past year. The engineers said that the company had raised output goals and had become less forgiving about deadlines. \[…] One Amazon engineer said his team was roughly half the size it had been last year, but it was expected to produce roughly the same amount of code by using A.I.

- Brian Merchant, “[The AI jobs crisis is here, now](https://www.bloodinthemachine.com/p/the-ai-jobs-crisis-is-here-now)”

> The AI jobs crisis does not, [as I’ve written before](https://gizmodo.com/robots-are-not-coming-for-your-job-management-is-1835127820#:~:text=Listen%3A%20'Robots'%20are%20not,on%20its%20comparatively%20superior%20merits.), look like sentient programs arising all around us, inexorably replacing human jobs en masse. It’s a series of management decisions being made by executives seeking to cut labor costs and consolidate control in their organizations. \[…]

> These imperatives have always existed, of course; bosses have historically tried to maximize profits by using cost-cutting technologies. But generative AI has been uniquely powerful in equipping them with a narrative with which to do so—and to thus justify degrading, disempowering, or destroying vulnerable jobs. 

- Brian Merchant, “[The ‘AI jobs apocalypse’ is for the bosses](https://www.bloodinthemachine.com/p/the-ai-jobs-apocalypse-is-for-the)”

In this article, Merchant points to a [blog post](https://www.axios.com/2025/05/20/ai-leadership) by Jim VandeHei, CEO of Axios, in which VandeHei says that he 

> recently told the Axios staff that we're done sugar-coating it, and see an urgent need for _every_ employee to turn AI into a force multiplier for their specific work. We then gave them tools to test. My exact words to a small group of our finance, legal and talent colleagues last week: “You are committing career suicide if you're not aggressively experimenting with AI.”

VandeHei adds that

> We tell most staff they should be spending 10% or more of their day using AI to discover ways to double their performance by the end of the year. Some, like coders, should shoot for 10x-ing productivity as AI improves.

In other words, as Merchant puts it

> The message is this: There is an AI jobs apocalypse coming, everything is going to change, and if you hope to survive it, you’re going to have to learn to be a lot more productive, for me, your boss.

### “Human-in-the-loop” as a “reverse centaur”

- Cory Doctorow, “[Pluralistic: “Humans in the loop” must detect the hardest-to-spot errors, at superhuman speed (23 Apr 2024)](https://pluralistic.net/2024/04/23/maximal-plausibility/)”

> Automation can _augment_ a worker. We can call this a “centaur” – the worker offloads a repetitive task, or one that requires a high degree of vigilance, or (worst of all) both. They're a human head on a robot body (hence “centaur”). Think of the sensor/vision system in your car that beeps if you activate your turn-signal while a car is in your blind spot. You're in charge, but you're getting a second opinion from the robot.

- Cory Doctorow, “[Pluralistic: Humans are not perfectly vigilant (01 Apr 2024)](https://pluralistic.net/2024/04/01/human-in-the-loop/)”

> This turns AI-“assisted” coders into _reverse_ centaurs. The AI can churn out code at superhuman speed, and you, the human in the loop, must maintain perfect vigilance and attention as you review that code, spotting the cleverly disguised hooks for malicious code that the AI can't be prevented from inserting into its code.

## Output Quality

- William Harding, Matthew Kloster, “[Coding on Copilot: 2023 Data Suggests Downward Pressure on Code Quality](https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality)”

> We find disconcerting trends for maintainability. Code churn -- the percentage of lines that are reverted or updated less than two weeks after being authored -- is projected to double in 2024 compared to its 2021, pre-AI baseline. We further find that the percentage of “added code” and “copy/pasted code” is increasing in proportion to “updated,” “deleted,” and “moved” code. In this regard, code generated during 2023 more resembles an itinerant contributor, prone to violate the [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)-ness of the repos visited.

## Harassment and Spam

- Samantha Cole, “[Schools Are Failing to Protect Students From Non-Consensual Deepfakes, Report Shows](https://www.404media.co/schools-are-failing-to-protect-students-from-non-consensual-deepfakes-report-shows/)”

> “Nudify” and “undress” apps are easy to use and find online and are contributing to the epidemic of explicit deepfakes among teenagers. [Last month Emanuel reported](https://www.404media.co/google-search-includes-paid-promotion-of-nudify-apps/) that Google was promoting these apps in search results: “Google Search didn’t only lead users to these harmful apps, but was also profiting from the apps which pay to place links against specific search terms,” he wrote.

- Emanuel Maiberg, “[No One Knows How to Deal With ‘Student-on-Student’ AI CSAM](https://www.404media.co/no-one-knows-how-to-deal-with-student-on-student-ai-csam/)”

> The report says that while children may recognize that AI-generating nonconsensual content is wrong they can assume “it’s legal, believing that if it were truly illegal, there wouldn’t be an app for it.” The report, which cites several 404 Media stories about this issue, notes that this normalization is in part a result of many “nudify” apps being available on the [Google and Apple app stores](https://www.404media.co/apple-removes-nonconsensual-ai-nude-apps-following-404-media-investigation/), and that their ability to AI-generate nonconsensual nudity is openly advertised to students on [Google](https://www.404media.co/google-search-includes-paid-promotion-of-nudify-apps/) and social media platforms like [Instagram](https://www.404media.co/instagram-ads-send-this-nudify-site-90-percent-of-its-traffic/) and TikTok

- Jason Koebler, “[‘What Was She Supposed to Report?:’ Police Report Shows How a High School Deepfake Nightmare Unfolded](https://www.404media.co/what-was-she-supposed-to-report-police-report-shows-how-a-high-school-deepfake-nightmare-unfolded/)” ([archived link](https://archive.ph/DXLIQ))

- Emanuel Maiberg, “[AI Images in Google Search Results Have Opened a Portal to Hell](https://www.404media.co/google-image-search-ai-results-have-opened-a-portal-to-hell/)” ([archived link](https://archive.ph/ys3ac))

> The news is yet another example of how the tools people have used to navigate the internet for decades are overwhelmed by the flood of AI-generated content even when they are not asking for it and which almost exclusively use people’s work or likeness without consent. At times, the deluge of AI content makes it difficult for users to differentiate between what is real and what is AI-generated.

## AI as an Accountability Sink

- Jonah Owen Lamb, “[San Francisco cops are using AI to write police reports](https://sfstandard.com/2025/06/10/sfpd-using-ai-police-reports/)”

> Guariglia added that defense lawyers across the country have told EFF that AI raises a major problem when it comes to the veracity of police testimony. Police reports can’t be presented as evidence alone in court, so officers must testify about what they wrote. But if AI wrote a report, and a cop’s testimony is different from that report, police will be able to blame the technology.

> “If a cop is caught in a lie on the stand, it’s much easier for them to say the AI made that up as opposed to them saying you caught me lying in the report,” Guariglia said.

> Another problem, he said, is that there is no way to track which part of a report was written by AI versus an officer, making it difficult to parse the document if questions are raised about its veracity.

> Some U.S. police departments have begun using AI but are not disclosing its use, Guariglia said.

- Atay Kozlovski, “[When Algorithms Decide Who is a Target: IDF's use of AI in Gaza](https://www.techpolicy.press/when-algorithms-decide-who-is-a-target-idfs-use-of-ai-in-gaza/)”

> This kind of problem stems from a high level of complexity in the algorithmic structure, which prevents even the designers of the AI system from fully understanding how or why a specific input leads to a specific output. Without such an explanation, it would not only be difficult to dispute the validity of any recommendation provided by the system, but it may also preclude us from holding any involved actor morally responsible as they would not have access to the necessary information required for questioning the output.

## AI, Ideology, and Power

- Mandy Brown, “[Toolmen](https://aworkinglibrary.com/writing/toolmen)”

> What AI is is an ideology…\[the] ideology itself is nothing new—it is the age-old system of supremacy, granting care and comfort to some while relegating others to servitude and penury—but the wrappings have been updated for the late capital, late digital age… Engaging with AI as a *technology* is to play the fool—it’s to observe the reflective surface of the thing without taking note of the way it sends roots deep down into the ground, breaking up bedrock, poisoning the soil, reaching far and wide to capture, uproot, strangle, and steal everything within its reach.

- Tracy Durnell, “[Generative AI and the Business Borg aesthetic](https://tracydurnell.com/2025/06/02/generative-ai-and-the-business-borg-aesthetic/)”

> Teri Kanefield [breaks down](https://terikanefield.com/your-brain-on-ideology/) Leor Zmigrod’s [book on ideology](https://us.macmillan.com/books/9781250344595/theideologicalbrain/), explaining that “All ideologies seek a utopia.” The Business Borg utopia puts billionaires and their ilk high atop society, in control via the technology they own.

> Core values I see uniting the Business Borg aesthetic are:
> 1. **only the output matters**
> 2. **efficiency is king**
> 3. **quantity over quality**
> 4. **appearance trumps reality**
> 5. **“progress” cannot be stopped** \[…]

> **Generative AI — both imagery and text — is inextricable from the corporate vision for its use: a world in which [workers are powerless and worthless](https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic), replaced by “free” generated material.** Corporate GenAI cannot be separated from the purpose for its use or the billionaires and billionaire-wannabes who shill for it.

- Ali Alkhatib, “[To Live in Their Utopia: Why Algorithmic Systems Create Absurd Outcomes](https://ali-alkhatib.com/research#utopia)”

> This paper draws from anthropological work on bureaucracies, states, and power, translating these ideas into a theory describing the structural tendency for powerful algorithmic systems to cause tremendous harm. I show how administrative models and projections of the world create marginalization, just as algorithmic models cause representational and allocative harm.

## Economics of AI

- Ed Zitron, “[The Rot-Com Bubble](https://www.wheresyoured.at/rotcombubble/)”

> As we speak, the tech industry is grappling with a mid-life crisis where it desperately searches for the next hyper-growth market, eagerly pushing customers and businesses to adopt technology that nobody asked for in the hopes that they can keep the Rot Economy alive.

- Ed Zitron, “[OpenAI Is a Bad Business](https://www.wheresyoured.at/oai-business/)”

> To be abundantly clear, as it stands, OpenAI currently spends $2.35 to make $1.

## AI and Climate

- Paige Lambermont, “[AI Boom Power Surge: Plants Revived, Fossil Fuels Reconsidered](https://www.independent.org/article/2025/05/08/ai-power-plants-gas-coal/)”

- Dara Kerr, “[AI brings soaring emissions for Google and Microsoft, a major contributor to climate change](https://www.npr.org/2024/07/12/g-s1-9545/ai-brings-soaring-emissions-for-google-and-microsoft-a-major-contributor-to-climate-change)

- [*DOE Releases New Report Evaluating Increase in Electricity Demand from Data Centers*](https://www.energy.gov/articles/doe-releases-new-report-evaluating-increase-electricity-demand-data-centers)

> The report finds that data centers consumed about 4.4% of total U.S. electricity in 2023 and are expected to consume approximately 6.7 to 12% of total U.S. electricity by 2028. The report indicates that total data center electricity usage climbed from 58 TWh in 2014 to 176 TWh in 2023 and estimates an increase between 325 to 580 TWh by 2028.

One might argue that 6.7% of U.S. electricity usage is a relatively small portion of contributions to climate change, as well as being from all data center usage, not just AI. However, especially given all the other negatives of LLMs and related technologies, I still do not see them as worth that level of energy consumption. See also Molly White, “[AI isn't useless. But is it worth it?](https://www.citationneeded.news/ai-isnt-useless/).”

<!-- - Marc Levy, “[Big Tech’s soaring energy demands are making coal-fired power plant sites attractive](https://apnews.com/article/coal-electricity-artificial-intelligence-trump-power-energy-a3779aee7970dabded42d8d7b3ef3783)” -->

## Rhetoric of AI Boosters

- Glyph, [*I Think I’m Done Thinking About genAI For Now*](https://blog.glyph.im/2025/06/i-think-im-done-thinking-about-genai-for-now.html)

> …every discussion is a [motte-and-bailey](https://en.wikipedia.org/wiki/Motte-and-bailey_fallacy). If I use a free model and get a bad result I’m told it’s because I should have used the paid model. If I get a bad result with ChatGPT I should have used Claude. If I get a bad result with a chatbot I need to start using an agentic tool. If an agentic tool deletes my hard drive by putting `os.system(“rm -rf ~/”)` into `sitecustomize.py` then I guess I should have built my own MCP integration with a completely novel heretofore never even considered security sandbox or something?

- Edward Ongweso Jr., “[The phony comforts of useful idiots](https://thetechbubble.substack.com/p/the-phony-comforts-of-useful-idiots)”

## Discussion Covering Multiple Facets

- Molly White, “[AI isn't useless. But is it worth it?](https://www.citationneeded.news/ai-isnt-useless/)”
