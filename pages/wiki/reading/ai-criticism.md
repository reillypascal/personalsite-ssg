---
title: AI Criticism Reading
description: Annotated reading list on AI criticism
date: 2025-05-25T13:14:00-0400
octothorpes:
  - 
tags:
  - wiki
  - reading
  - ai
  - politics
---

## Bullets
- Low-quality outputs, e.g., “[downward pressure on code quality](#coding-on-copilot%3A-2023-data-suggests-downward-pressure-on-code-quality)”
- Accountability sink; [tool to reinscribe existing social biases](#to-live-in-their-utopia%3A-why-algorithmic-systems-create-absurd-outcomes)
- It's a [bubble driven by the demand for more tech hypergrowth markets](#economics); currently dragging down the rest of tech, too
<!-- - Concentrates control over computation: models take a huge amount of computational power to train and run; inflates demand for cloud computing infrastructure
- Invisible labor: classification, labeling, RLHF
- At the top, driven by a desire to replace workers or have a bargaining chip to use against us
- Environmental impact: missed and stretched climate goals, reopening coal plants
- The model itself is a product built on stolen labor; not just about whether the output is similar to a human learning from other art -->

## Output Quality

### [Coding on Copilot: 2023 Data Suggests Downward Pressure on Code Quality](https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality)

> We find disconcerting trends for maintainability. Code churn -- the percentage of lines that are reverted or updated less than two weeks after being authored -- is projected to double in 2024 compared to its 2021, pre-AI baseline. We further find that the percentage of “added code” and “copy/pasted code” is increasing in proportion to “updated,” “deleted,” and “moved” code. In this regard, code generated during 2023 more resembles an itinerant contributor, prone to violate the [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)-ness of the repos visited.

## Political and Social Implications

### [To Live in Their Utopia: Why Algorithmic Systems Create Absurd Outcomes](https://ali-alkhatib.com/research#utopia)

> The promise AI’s proponents have made for decades is one in which our needs are predicted, anticipated, and met - often before we even realize it. Instead, algorithmic systems, particularly AIs trained on large datasets and deployed to massive scales, seem to keep making the wrong decisions, causing harm and rewarding absurd outcomes. Attempts to make sense of why AIs make wrong calls in the moment explain the instances of errors, but how the environment surrounding these systems precipitate those instances remains murky. This paper draws from anthropological work on bureaucracies, states, and power, translating these ideas into a theory describing the structural tendency for powerful algorithmic systems to cause tremendous harm. I show how administrative models and projections of the world create marginalization, just as algorithmic models cause representational and allocative harm. This paper concludes with a recommendation to avoid the absurdity algorithmic systems produce by denying them power.

## Economics

### Ed Zitron
- [*OpenAI Is a Bad Business*](https://www.wheresyoured.at/oai-business/)

> To be abundantly clear, as it stands, OpenAI currently spends $2.35 to make $1.

- [*The Rot-Com Bubble*](https://www.wheresyoured.at/rotcombubble/)

> As we speak, the tech industry is grappling with a mid-life crisis where it desperately searches for the next hyper-growth market, eagerly pushing customers and businesses to adopt technology that nobody asked for in the hopes that they can keep the Rot Economy alive.