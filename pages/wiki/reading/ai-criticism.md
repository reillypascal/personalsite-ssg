---
title: AI — Critical Writing and Reporting
description: Reading list of sources critical of AI or reporting on harm caused by its use
date: 2025-05-25T13:14:00-0400
octothorpes:
  - ai
tags:
  - wiki
  - reading
  - ai
  - politics
---

This page is a collection of reporting, research, and essays critical of large language models (LLMs) and other “generative AI.” Articles are organized based on topic. It is an ever-growing list.

## Output Quality

- William Harding, Matthew Kloster, “[Coding on Copilot: 2023 Data Suggests Downward Pressure on Code Quality](https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality)”

> We find disconcerting trends for maintainability. Code churn -- the percentage of lines that are reverted or updated less than two weeks after being authored -- is projected to double in 2024 compared to its 2021, pre-AI baseline. We further find that the percentage of “added code” and “copy/pasted code” is increasing in proportion to “updated,” “deleted,” and “moved” code. In this regard, code generated during 2023 more resembles an itinerant contributor, prone to violate the [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)-ness of the repos visited.

## AI as a Labor Issue

- Noam Scheiber, “[At Amazon, Some Coders Say Their Jobs Have Begun to Resemble Warehouse Work](https://www.nytimes.com/2025/05/25/business/amazon-ai-coders.html)” ([archived link](https://archive.is/g127S))

> Three Amazon engineers said that managers had increasingly pushed them to use A.I. in their work over the past year. The engineers said that the company had raised output goals and had become less forgiving about deadlines. \[…] One Amazon engineer said his team was roughly half the size it had been last year, but it was expected to produce roughly the same amount of code by using A.I.

## Harassment and Spam

- Samantha Cole, “[Schools Are Failing to Protect Students From Non-Consensual Deepfakes, Report Shows](https://www.404media.co/schools-are-failing-to-protect-students-from-non-consensual-deepfakes-report-shows/)”

> “Nudify” and “undress” apps are easy to use and find online and are contributing to the epidemic of explicit deepfakes among teenagers. [Last month Emanuel reported](https://www.404media.co/google-search-includes-paid-promotion-of-nudify-apps/) that Google was promoting these apps in search results: “Google Search didn’t only lead users to these harmful apps, but was also profiting from the apps which pay to place links against specific search terms,” he wrote.

- Jason Koebler, “[‘What Was She Supposed to Report?:’ Police Report Shows How a High School Deepfake Nightmare Unfolded](https://www.404media.co/what-was-she-supposed-to-report-police-report-shows-how-a-high-school-deepfake-nightmare-unfolded/)” ([archived link](https://archive.ph/DXLIQ))

- Emanuel Maiberg, “[AI Images in Google Search Results Have Opened a Portal to Hell](https://www.404media.co/google-image-search-ai-results-have-opened-a-portal-to-hell/)” ([archived link](https://archive.ph/ys3ac))

> The news is yet another example of how the tools people have used to navigate the internet for decades are overwhelmed by the flood of AI-generated content even when they are not asking for it and which almost exclusively use people’s work or likeness without consent. At times, the deluge of AI content makes it difficult for users to differentiate between what is real and what is AI-generated.

## AI as an Accountability Sink

- Atay Kozlovski, “[When Algorithms Decide Who is a Target: IDF's use of AI in Gaza](https://www.techpolicy.press/when-algorithms-decide-who-is-a-target-idfs-use-of-ai-in-gaza/)”

> This kind of problem stems from a high level of complexity in the algorithmic structure, which prevents even the designers of the AI system from fully understanding how or why a specific input leads to a specific output. Without such an explanation, it would not only be difficult to dispute the validity of any recommendation provided by the system, but it may also preclude us from holding any involved actor morally responsible as they would not have access to the necessary information required for questioning the output.

## AI and Power

- Ali Alkhatib, “[To Live in Their Utopia: Why Algorithmic Systems Create Absurd Outcomes](https://ali-alkhatib.com/research#utopia)”

> This paper draws from anthropological work on bureaucracies, states, and power, translating these ideas into a theory describing the structural tendency for powerful algorithmic systems to cause tremendous harm. I show how administrative models and projections of the world create marginalization, just as algorithmic models cause representational and allocative harm.

## Economics of AI

- Ed Zitron, “[The Rot-Com Bubble](https://www.wheresyoured.at/rotcombubble/)”

> As we speak, the tech industry is grappling with a mid-life crisis where it desperately searches for the next hyper-growth market, eagerly pushing customers and businesses to adopt technology that nobody asked for in the hopes that they can keep the Rot Economy alive.

- Ed Zitron, “[OpenAI Is a Bad Business](https://www.wheresyoured.at/oai-business/)”

> To be abundantly clear, as it stands, OpenAI currently spends $2.35 to make $1.

## Discussion Covering Multiple Facets

- Molly White, “[AI isn't useless. But is it worth it?](https://www.citationneeded.news/ai-isnt-useless/)”

- Edward Ongweso Jr., “[The phony comforts of useful idiots](https://thetechbubble.substack.com/p/the-phony-comforts-of-useful-idiots)”